{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaPaaG1Fuo0G"
   },
   "source": [
    "# CSE 416 Homework 6: ML Kaggle Exercise\n",
    "Team Name: BACmono \\\n",
    "Team Member: Bob Lin, Angela Zhao, Chris Chen\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59HieX0OW5ar"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pg-mKiSSXHYe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVjxLwAtXMYt"
   },
   "source": [
    "## Read Dataset and Manipulate with Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wMPLJG2WXciq"
   },
   "outputs": [],
   "source": [
    "edx_train_data = pd.read_csv('edx_train.csv')\n",
    "edx_test_data = pd.read_csv('edx_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB1Dnb1rv_yS"
   },
   "source": [
    "Check the count of null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "he50KSeFTEcI",
    "outputId": "95c2f97d-6a5f-454a-8fa9-d5a839cf1e1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "course_id               0\n",
       "userid_DI               0\n",
       "registered              0\n",
       "viewed                  0\n",
       "explored                0\n",
       "certified               0\n",
       "final_cc_cname_DI       0\n",
       "LoE_DI                966\n",
       "YoB                   914\n",
       "gender                775\n",
       "grade                 370\n",
       "start_time_DI           0\n",
       "last_event_DI        1508\n",
       "nevents              1639\n",
       "ndays_act            1639\n",
       "nplay_video          6747\n",
       "nchapters            2563\n",
       "nforum_posts            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edx_train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoqEp5LQxLW4"
   },
   "source": [
    "Note that there are many columns with null values, and each column has different type and meaning, so we cannot simply delete rows with the null. Therefore, we analyzed the property and real-life implication of each column, and then implement corresponding manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KspfXw5kxrCf"
   },
   "source": [
    "**1. Fill in the null values of numerical variables** \\\\\n",
    "For `nevent`, `ndays_act`, `nplay_video`, and `nchapters`, we fill the null values with 0 because the description file says that blank `nevent` if zero no interaction beyond registration, and we assumed the rest of variables beginning with `n` have the same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p4PTp6IHyxhT"
   },
   "outputs": [],
   "source": [
    "for value in ['nevents', 'ndays_act', 'nplay_video', 'nchapters']:\n",
    "  edx_train_data[value] = edx_train_data[value].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ_a7jADzE8W"
   },
   "source": [
    "**2. Fill in the null values of categorical variables** \\\\\n",
    "`gender` and `LoE_DI` are categorical variables. According to the spec, `gender` has three values--'male', 'female', and 'other'. Since all the non-null data are either 'male' or 'female', we fill in the null with 'other' ('o' as the value). Then `LoE_DI` is also a categorical variable, but all the possible values appear in the spec appears in this table, so we need to create an 'other' for the null values. But later we will remove this column after we get dummies, so that this value will not have any influence on the prediction result since it's not something we should see according to the spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2IaqwtsIzEKO"
   },
   "outputs": [],
   "source": [
    "for value2 in ['LoE_DI', 'gender']: \n",
    "  edx_train_data[value2] = edx_train_data[value2].fillna('o')\n",
    "\n",
    "average_yob = math.floor(edx_train_data['YoB'].mean()) * 1.0\n",
    "edx_train_data['YoB'] = edx_train_data['YoB'].fillna(average_yob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKNMi7GkzRdl"
   },
   "source": [
    "**3. Fill in the null values of should-be numerical variables**  \\\\\n",
    "Because there are some blank spaces in `grade`, all values of this variable are string rather than numeric. Therefore, we firstly remove all empty spaces in `grade` and then turn them into numeric variables such that `get_dummies()` won't treat them as categorical variables and put them into separate columns. We will fill in the null values with mean grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JM6vGAp6zmhf"
   },
   "outputs": [],
   "source": [
    "edx_train_data['grade'] = edx_train_data['grade'].str.strip()\n",
    "data = edx_train_data['grade'].tolist()\n",
    "sum = 0.0\n",
    "count = len(edx_train_data['grade'])\n",
    "minus = 0\n",
    "for value3 in data: \n",
    "  if isinstance(value3, str) and len(value3.strip()) != 0:\n",
    "    sum = sum + float(value3)\n",
    "  else: \n",
    "    minus = minus + 1\n",
    "average_grade = sum/(count - minus)\n",
    "edx_train_data['grade'] = pd.to_numeric(edx_train_data['grade'])\n",
    "edx_train_data['grade'] = edx_train_data['grade'].fillna(average_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCbMlzWtzUxH"
   },
   "source": [
    "**4. Fill in the null values of date variables** \\\\\n",
    "It is hard to directly analyze the start date and end date, but the duration is more useful for analysis. After we checked the null values for each columns, we can see that start dates are all filled whereas end dates are not. So, if both start and end dates exist, `duration` is just the difference between the two; otherwise, it is filled by the average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mXdRI1SslLG8"
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "count = 0\n",
    "day_list = []\n",
    "for value4 in range(edx_train_data.shape[0]):\n",
    "  d1 = edx_train_data['start_time_DI'][value4]\n",
    "  d2 = edx_train_data['last_event_DI'][value4]\n",
    "  if isinstance(d2, str):\n",
    "    d1 = datetime.strptime(d1, '%m/%d/%y')\n",
    "    d2 = datetime.strptime(d2, '%m/%d/%y')\n",
    "    day = abs(d1 - d2).days\n",
    "    sum += day\n",
    "    count = count + 1\n",
    "    day_list.append(day)\n",
    "  else:\n",
    "    day_list.append(np.nan)\n",
    "average_duration = sum / count\n",
    "\n",
    "edx_train_data['duration'] = day_list\n",
    "edx_train_data['duration'] = edx_train_data['duration'].fillna(average_duration)\n",
    "edx_train_data = edx_train_data.drop(['start_time_DI', 'last_event_DI'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptQN60fj43_3"
   },
   "source": [
    "## Feature Selection\n",
    "Since `certified` is our target of prediction, and `userid_DI` is unique to every user and doesn't have analysis value, we removed these two variables from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqmxV9hndkMM",
    "outputId": "5df603a4-ee62-4573-d6b6-54327b20ca9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['course_id',\n",
       " 'registered',\n",
       " 'viewed',\n",
       " 'explored',\n",
       " 'final_cc_cname_DI',\n",
       " 'LoE_DI',\n",
       " 'YoB',\n",
       " 'gender',\n",
       " 'grade',\n",
       " 'nevents',\n",
       " 'ndays_act',\n",
       " 'nplay_video',\n",
       " 'nchapters',\n",
       " 'nforum_posts',\n",
       " 'duration']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = edx_train_data.columns.to_list()\n",
    "if 'certified' in features and 'userid_DI' in features:\n",
    "  features.remove('certified')\n",
    "  features.remove('userid_DI')\n",
    "  target = 'certified'\n",
    "  edx_train_y = edx_train_data[target]\n",
    "\n",
    "edx_train_data = edx_train_data[features]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uhMScWN7-a8"
   },
   "source": [
    "To make sure categorical variables are trained in classification models, we turn them into numerical variables using `get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MUyRQoUyCjpY"
   },
   "outputs": [],
   "source": [
    "edx_train_data = pd.get_dummies(edx_train_data)\n",
    "features = list(edx_train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bslHiynYu9pO",
    "outputId": "fd600820-e082-4c82-ddd8-82bf168536cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['registered',\n",
       " 'viewed',\n",
       " 'explored',\n",
       " 'YoB',\n",
       " 'grade',\n",
       " 'nevents',\n",
       " 'ndays_act',\n",
       " 'nplay_video',\n",
       " 'nchapters',\n",
       " 'nforum_posts',\n",
       " 'duration',\n",
       " 'course_id_HarvardX/CB22x/2013_Spring',\n",
       " 'course_id_HarvardX/CS50x/2012',\n",
       " 'course_id_HarvardX/ER22x/2013_Spring',\n",
       " 'course_id_HarvardX/PH207x/2012_Fall',\n",
       " 'course_id_HarvardX/PH278x/2013_Spring',\n",
       " 'final_cc_cname_DI_Australia',\n",
       " 'final_cc_cname_DI_Bangladesh',\n",
       " 'final_cc_cname_DI_Brazil',\n",
       " 'final_cc_cname_DI_Canada',\n",
       " 'final_cc_cname_DI_China',\n",
       " 'final_cc_cname_DI_Colombia',\n",
       " 'final_cc_cname_DI_Egypt',\n",
       " 'final_cc_cname_DI_France',\n",
       " 'final_cc_cname_DI_Germany',\n",
       " 'final_cc_cname_DI_Greece',\n",
       " 'final_cc_cname_DI_India',\n",
       " 'final_cc_cname_DI_Indonesia',\n",
       " 'final_cc_cname_DI_Japan',\n",
       " 'final_cc_cname_DI_Mexico',\n",
       " 'final_cc_cname_DI_Morocco',\n",
       " 'final_cc_cname_DI_Nigeria',\n",
       " 'final_cc_cname_DI_Other Africa',\n",
       " 'final_cc_cname_DI_Other East Asia',\n",
       " 'final_cc_cname_DI_Other Europe',\n",
       " 'final_cc_cname_DI_Other Middle East/Central Asia',\n",
       " 'final_cc_cname_DI_Other North & Central Amer., Caribbean',\n",
       " 'final_cc_cname_DI_Other Oceania',\n",
       " 'final_cc_cname_DI_Other South America',\n",
       " 'final_cc_cname_DI_Other South Asia',\n",
       " 'final_cc_cname_DI_Pakistan',\n",
       " 'final_cc_cname_DI_Philippines',\n",
       " 'final_cc_cname_DI_Poland',\n",
       " 'final_cc_cname_DI_Portugal',\n",
       " 'final_cc_cname_DI_Russian Federation',\n",
       " 'final_cc_cname_DI_Spain',\n",
       " 'final_cc_cname_DI_Ukraine',\n",
       " 'final_cc_cname_DI_United Kingdom',\n",
       " 'final_cc_cname_DI_United States',\n",
       " 'final_cc_cname_DI_Unknown/Other',\n",
       " \"LoE_DI_Bachelor's\",\n",
       " 'LoE_DI_Doctorate',\n",
       " 'LoE_DI_Less than Secondary',\n",
       " \"LoE_DI_Master's\",\n",
       " 'LoE_DI_Secondary',\n",
       " 'gender_f',\n",
       " 'gender_m',\n",
       " 'gender_o']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.remove('LoE_DI_o')\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWg7Ysq-eWhm"
   },
   "source": [
    "Split the training and validation set with a proportion of 80:20 with random state of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ayCYfiWtgeau"
   },
   "outputs": [],
   "source": [
    "X = edx_train_data\n",
    "y = edx_train_y\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mdq8tCb4efN3"
   },
   "source": [
    "In class, we've discussed that Lasso regression is best used for feature selection. We trained several Lasso models with different `lambda` and chose the one with the least validation error. At last, we implemented the Lasso model with lambda=0.000001 and created a dataframe with feature and its coefficient learned from Lasso for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuOb_aRoPtOn",
    "outputId": "0aea7aef-318b-4dc0-bccc-05341d252a28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-15, 1.e-14, 1.e-13, 1.e-12, 1.e-11, 1.e-10, 1.e-09, 1.e-08,\n",
       "       1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00,\n",
       "       1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-15,5,21,base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6Pfb91WgX3Z",
    "outputId": "7ab74020-3248-4633-a416-8886ea0a4839"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.08185273525001, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.992234465035516, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.971930087144898, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.971991773982651, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.971981604142712, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.971981280992864, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.971984152455974, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.972013921955934, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.548434014881302, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7575231511303144, tolerance: 0.17318538395660868\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lamb = np.logspace(-15,5,21,base=10)\n",
    "lasso_data = []\n",
    "for i in range(len(lamb)):\n",
    "    l1_model = Lasso(alpha=lamb[i], random_state=0)\n",
    "    l1_model.fit(X_train, y_train)\n",
    "    y_pred = l1_model.predict(X_train)\n",
    "    train_rmse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    y_pred = l1_model.predict(X_valid)\n",
    "    validation_rmse = mean_squared_error(y_valid, y_pred, squared=False)\n",
    "    lasso_data.append({\n",
    "        'l1_penalty': lamb[i],\n",
    "        'model': l1_model,\n",
    "        'train_rmse': train_rmse,\n",
    "        'validation_rmse': validation_rmse\n",
    "    })\n",
    "lasso_data = pd.DataFrame(lasso_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "Jp-bgBqx2Qf1",
    "outputId": "c69a39e1-df47-486b-a5e3-5de001de5670"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1_penalty</th>\n",
       "      <th>model</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>validation_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-15</td>\n",
       "      <td>Lasso(alpha=1e-15, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-14</td>\n",
       "      <td>Lasso(alpha=1e-14, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-13</td>\n",
       "      <td>Lasso(alpha=1e-13, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>Lasso(alpha=1e-12, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-11</td>\n",
       "      <td>Lasso(alpha=1e-11, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>Lasso(alpha=1e-10, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>Lasso(alpha=1e-09, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>Lasso(alpha=1e-08, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>Lasso(alpha=1e-07, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>Lasso(alpha=1e-06, random_state=0)</td>\n",
       "      <td>0.121752</td>\n",
       "      <td>0.115814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>Lasso(alpha=1e-05, random_state=0)</td>\n",
       "      <td>0.121753</td>\n",
       "      <td>0.115820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>Lasso(alpha=0.0001, random_state=0)</td>\n",
       "      <td>0.121828</td>\n",
       "      <td>0.115939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>Lasso(alpha=0.001, random_state=0)</td>\n",
       "      <td>0.122428</td>\n",
       "      <td>0.116029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>Lasso(alpha=0.01, random_state=0)</td>\n",
       "      <td>0.131689</td>\n",
       "      <td>0.123413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>Lasso(alpha=0.1, random_state=0)</td>\n",
       "      <td>0.265473</td>\n",
       "      <td>0.249261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>Lasso(random_state=0)</td>\n",
       "      <td>0.292281</td>\n",
       "      <td>0.275024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>Lasso(alpha=10.0, random_state=0)</td>\n",
       "      <td>0.395367</td>\n",
       "      <td>0.386238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>Lasso(alpha=100.0, random_state=0)</td>\n",
       "      <td>0.430923</td>\n",
       "      <td>0.428351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>Lasso(alpha=1000.0, random_state=0)</td>\n",
       "      <td>0.497188</td>\n",
       "      <td>0.495591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>Lasso(alpha=10000.0, random_state=0)</td>\n",
       "      <td>0.497188</td>\n",
       "      <td>0.495591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>Lasso(alpha=100000.0, random_state=0)</td>\n",
       "      <td>0.497188</td>\n",
       "      <td>0.495591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      l1_penalty                                  model  train_rmse  \\\n",
       "0   1.000000e-15     Lasso(alpha=1e-15, random_state=0)    0.121752   \n",
       "1   1.000000e-14     Lasso(alpha=1e-14, random_state=0)    0.121752   \n",
       "2   1.000000e-13     Lasso(alpha=1e-13, random_state=0)    0.121752   \n",
       "3   1.000000e-12     Lasso(alpha=1e-12, random_state=0)    0.121752   \n",
       "4   1.000000e-11     Lasso(alpha=1e-11, random_state=0)    0.121752   \n",
       "5   1.000000e-10     Lasso(alpha=1e-10, random_state=0)    0.121752   \n",
       "6   1.000000e-09     Lasso(alpha=1e-09, random_state=0)    0.121752   \n",
       "7   1.000000e-08     Lasso(alpha=1e-08, random_state=0)    0.121752   \n",
       "8   1.000000e-07     Lasso(alpha=1e-07, random_state=0)    0.121752   \n",
       "9   1.000000e-06     Lasso(alpha=1e-06, random_state=0)    0.121752   \n",
       "10  1.000000e-05     Lasso(alpha=1e-05, random_state=0)    0.121753   \n",
       "11  1.000000e-04    Lasso(alpha=0.0001, random_state=0)    0.121828   \n",
       "12  1.000000e-03     Lasso(alpha=0.001, random_state=0)    0.122428   \n",
       "13  1.000000e-02      Lasso(alpha=0.01, random_state=0)    0.131689   \n",
       "14  1.000000e-01       Lasso(alpha=0.1, random_state=0)    0.265473   \n",
       "15  1.000000e+00                  Lasso(random_state=0)    0.292281   \n",
       "16  1.000000e+01      Lasso(alpha=10.0, random_state=0)    0.395367   \n",
       "17  1.000000e+02     Lasso(alpha=100.0, random_state=0)    0.430923   \n",
       "18  1.000000e+03    Lasso(alpha=1000.0, random_state=0)    0.497188   \n",
       "19  1.000000e+04   Lasso(alpha=10000.0, random_state=0)    0.497188   \n",
       "20  1.000000e+05  Lasso(alpha=100000.0, random_state=0)    0.497188   \n",
       "\n",
       "    validation_rmse  \n",
       "0          0.115813  \n",
       "1          0.115813  \n",
       "2          0.115813  \n",
       "3          0.115813  \n",
       "4          0.115813  \n",
       "5          0.115813  \n",
       "6          0.115813  \n",
       "7          0.115813  \n",
       "8          0.115813  \n",
       "9          0.115814  \n",
       "10         0.115820  \n",
       "11         0.115939  \n",
       "12         0.116029  \n",
       "13         0.123413  \n",
       "14         0.249261  \n",
       "15         0.275024  \n",
       "16         0.386238  \n",
       "17         0.428351  \n",
       "18         0.495591  \n",
       "19         0.495591  \n",
       "20         0.495591  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_l2cMvFg8T39"
   },
   "outputs": [],
   "source": [
    "def df_coefficients(model, features):\n",
    "    \"\"\"\n",
    "    This function takes in a model column and a features column. \n",
    "    And constructs a dataframe with feature and its coefficient.\n",
    "    \"\"\"\n",
    "    feats = list(zip(features, model.coef_))\n",
    "    coef_df = pd.DataFrame(feats)\n",
    "    coef_df = coef_df.rename(columns={0:'feature',1:'coefficient'})\n",
    "    return coef_df\n",
    "    \n",
    "index = 9\n",
    "l1 = lasso_data.loc[index]\n",
    "zero_coef = abs(l1['model'].coef_) > 10 ** -2 * 4\n",
    "\n",
    "coef_df = df_coefficients(l1['model'], features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qCg_PvFmy-c"
   },
   "source": [
    "We found all features with nonzero coefficient and decided to use the most important 6 features to make predictions: `explored`, `grade`, `course_id_HarvardX/PH207x/2012_Fall`, `course_id_HarvardX/PH278x/2013_Spring`, `final_cc_cname_DI_Greece`, and `final_cc_cname_DI_Other Oceania`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "c_7kW8hfDnoA",
    "outputId": "9541d688-1334-437c-f5df-caed41dc0118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explored</td>\n",
       "      <td>0.085004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grade</td>\n",
       "      <td>0.882290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>course_id_HarvardX/PH207x/2012_Fall</td>\n",
       "      <td>-0.040044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>course_id_HarvardX/PH278x/2013_Spring</td>\n",
       "      <td>0.061228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>final_cc_cname_DI_Greece</td>\n",
       "      <td>0.045714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>final_cc_cname_DI_Other Oceania</td>\n",
       "      <td>-0.046482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 feature  coefficient\n",
       "0                               explored     0.085004\n",
       "1                                  grade     0.882290\n",
       "2    course_id_HarvardX/PH207x/2012_Fall    -0.040044\n",
       "3  course_id_HarvardX/PH278x/2013_Spring     0.061228\n",
       "4               final_cc_cname_DI_Greece     0.045714\n",
       "5        final_cc_cname_DI_Other Oceania    -0.046482"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features = []\n",
    "for num in range(len(zero_coef)):\n",
    "  if zero_coef[num] == True:\n",
    "    new_features.append(features[num])\n",
    "\n",
    "new_feat_df = coef_df.loc[coef_df['feature'].isin(new_features)]\n",
    "new_feat_df = new_feat_df.reset_index(drop=True)\n",
    "important_features_index = new_feat_df['coefficient'].nlargest(5).index\n",
    "important_features = new_feat_df.iloc[important_features_index]\n",
    "\n",
    "print('Number of selected features:', len(new_feat_df))\n",
    "new_feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJO76CoBiGzl"
   },
   "source": [
    "## Classifer 1: Majority Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3Fg7rT5E_Pb"
   },
   "source": [
    "We first create a majority classier and compute its accuracy as the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hH5ydERJiJ1y"
   },
   "outputs": [],
   "source": [
    "sum = edx_train_y.sum()/edx_train_data.shape[0]\n",
    "major_label = 1\n",
    "if sum < 0.5:\n",
    "  major_label = 0\n",
    "true_pred = y_valid.value_counts()[major_label]\n",
    "majority_classifier_validation_accuracy = true_pred / len(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2WxnWJmFPrU"
   },
   "source": [
    "The accuracy is 0.5679223744292238, approximately 57%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdXzdLttBf10"
   },
   "source": [
    "## Classifier 2: Adaboost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1kIo4inFWOM"
   },
   "source": [
    "Then we create an adaboost classifier and compute its accuracy. We expect it to be pretty high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOeOsRBsBkn4",
    "outputId": "145236fe-ef90-4cf1-c372-721677f5f8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost classifier training score: 0.9995717956037682\n",
      "Adaboost classifier validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "adaboost_model.fit(X_train[new_features],y_train)\n",
    "y_pred = adaboost_model.predict(X_valid[new_features])\n",
    "training_score = adaboost_model.score(X_train[new_features],y_train)\n",
    "a_validation_score = adaboost_model.score(X_valid[new_features],y_valid)\n",
    "\n",
    "print('Adaboost classifier training score:',training_score)\n",
    "print('Adaboost classifier validation score:',a_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDWz9s-_8YZp"
   },
   "source": [
    "Good lord the score is 1! But can we improve the performance by using `GridSearchCV`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giX495qPOPC2",
    "outputId": "e0ed1fe5-a9de-44d9-cb65-fb36a686e968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 25}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "  'n_estimators': [10,25,50,100,150,200],\n",
    "  'learning_rate': np.logspace(-4,0,5,base=10),\n",
    "  'algorithm': ['SAMME','SAMME.R']\n",
    "}\n",
    "search = GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyperparameters, \\\n",
    "                      cv=6,return_train_score=True)\n",
    "search.fit(X_train[new_features],y_train)\n",
    "\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5ipnGLSR_be",
    "outputId": "4cd47919-a087-45a0-b209-db1292032ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost classifier training score: 0.9995717956037682\n",
      "Adaboost classifier validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "adaboost_model = AdaBoostClassifier(n_estimators=25, random_state=0)\n",
    "adaboost_model.fit(X_train[new_features],y_train)\n",
    "y_pred = adaboost_model.predict(X_valid[new_features])\n",
    "training_score = adaboost_model.score(X_train[new_features],y_train)\n",
    "a_validation_score = adaboost_model.score(X_valid[new_features],y_valid)\n",
    "\n",
    "print('Adaboost classifier training score:',training_score)\n",
    "print('Adaboost classifier validation score:',a_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI5YbTh0GFv-"
   },
   "source": [
    "No obvious improvement. Should I be sad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf-fwF7BQm8K"
   },
   "source": [
    "## Classifier 3: K-Neareast Neighbor Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Crzwr7B5GSKb"
   },
   "source": [
    "We will then create a k-NN classifier to see how it performs. We also expect it to be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liycTpsMRn_g",
    "outputId": "8b4c6929-653c-4441-a3b6-88a770ac981a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15-NN classifier training score: 0.9997145304025121\n",
      "15-NN classifier validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "knn15_model = KNeighborsClassifier(n_neighbors = 15, weights = 'distance')\n",
    "knn15_model.fit(X_train[new_features], y_train)\n",
    "y_pred_knn = knn15_model.predict(X_valid[new_features])\n",
    "knn15_train_score = knn15_model.score(X_train[new_features], y_train)\n",
    "knn15_validation_score = knn15_model.score(X_valid[new_features], y_valid)\n",
    "\n",
    "print('15-NN classifier training score:', knn15_train_score)\n",
    "print('15-NN classifier validation score:', knn15_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FShWctJ-GbEL"
   },
   "source": [
    "Oh its score is also 1! Let's try to search for a better parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AprDeyMxwmyT",
    "outputId": "64623c8f-47ee-4c36-a65d-6e965221a426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 4}\n"
     ]
    }
   ],
   "source": [
    "knn_hyperparameters = {\n",
    "  'n_neighbors': np.linspace(1, 30, num = 30).astype(int),\n",
    "}\n",
    "knn_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = knn_hyperparameters, \\\n",
    "                      cv = 10, return_train_score = True)\n",
    "knn_search.fit(X_train[new_features], y_train)\n",
    "print(knn_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A9kA8b3KKWc"
   },
   "source": [
    "It turns out the best k is k = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8P-WPz7Gsli",
    "outputId": "0ff1eaa7-9b1b-4c32-af51-5060cdeba007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-NN classifier training score: 0.9997145304025121\n",
      "4-NN classifier validation score: 1.0\n"
     ]
    }
   ],
   "source": [
    "knn4_model = KNeighborsClassifier(n_neighbors = 4, weights = 'distance')\n",
    "knn4_model.fit(X_train[new_features], y_train)\n",
    "y_pred_knn = knn4_model.predict(X_valid[new_features])\n",
    "knn4_train_score = knn4_model.score(X_train[new_features], y_train)\n",
    "knn4_validation_score = knn4_model.score(X_valid[new_features], y_valid)\n",
    "\n",
    "print('4-NN classifier training score:', knn4_train_score)\n",
    "print('4-NN classifier validation score:', knn4_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYblVwTSGwU9"
   },
   "source": [
    "No obvious improvement. I guess I'm happy though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF4UrCJ0Rkrc"
   },
   "source": [
    "## Classifier 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gqWQduVG2R8"
   },
   "source": [
    "Lastly, we will try a logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iG6zqIfPRnWs",
    "outputId": "90da1dab-b280-49f9-e9bb-8fdce9a68b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic classifier training score: 0.9950042820439623\n",
      "Logistic classifier validation score: 0.9948630136986302\n"
     ]
    }
   ],
   "source": [
    "lgr_model = LogisticRegression(penalty='l2', random_state=1)\n",
    "lgr_model.fit(X_train[new_features],y_train)\n",
    "y_pred = lgr_model.predict(X_valid[new_features])\n",
    "training_score = lgr_model.score(X_train[new_features],y_train)\n",
    "l_validation_score = lgr_model.score(X_valid[new_features],y_valid)\n",
    "\n",
    "print('Logistic classifier training score:',training_score)\n",
    "print('Logistic classifier validation score:',l_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDeQFL8bG5vu"
   },
   "source": [
    "Search for better parameters and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGCOIek8ScD0",
    "outputId": "1f7d17b5-c906-468e-c9ab-755929de700a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "  'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "search = GridSearchCV(estimator=LogisticRegression(),param_grid=hyperparameters, \\\n",
    "                      cv=6,return_train_score=True)\n",
    "search.fit(X_train[new_features],y_train)\n",
    "\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_MDYnh5TR7x",
    "outputId": "18ac6f02-ec99-48e5-8d37-d25c7c85d612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic classifier training score: 0.9950042820439623\n",
      "Logistic classifier validation score: 0.9948630136986302\n"
     ]
    }
   ],
   "source": [
    "lgr_model = LogisticRegression(penalty='l2', solver='newton-cg', random_state=1)\n",
    "lgr_model.fit(X_train[new_features],y_train)\n",
    "y_pred = lgr_model.predict(X_valid[new_features])\n",
    "training_score = lgr_model.score(X_train[new_features],y_train)\n",
    "l_validation_score = lgr_model.score(X_valid[new_features],y_valid)\n",
    "\n",
    "print('Logistic classifier training score:',training_score)\n",
    "print('Logistic classifier validation score:',l_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jr-pzJmG_5z"
   },
   "source": [
    "Ok, no obvious improvement, and the score is lower than adaboost and k-NN. Bye logistic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9t94e-Uhgzf"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvJ0Fa8NHJ8g"
   },
   "source": [
    "We will show graphically how good the models perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "lVzwbmllXBr7",
    "outputId": "88e0cda5-6cb0-43f9-d666-c8438110a8dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = ['Majority', 'Adaboost', 'Kernal', 'Logistic']\n",
    "y = [majority_classifier_validation_accuracy, a_validation_score, knn4_validation_score, l_validation_score]\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOqK1dKGoZAJ"
   },
   "source": [
    "From the diagram above, all of AdaBoost, k-NN, and Logistic models work very well--over 99 percent accuracy. Among these three models, AdaBoost and k-NN has the highest validation accuracy, which is 1; and since k-NN is the most recently learned model, we will choose this as our main model.\n",
    "\n",
    "We tried to improve our prediction by using `GridSearchCV()`, but no obvious improvement is made. I guess you cannot be more correct than all correct, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBVP2bKclfVY"
   },
   "source": [
    "## Dealing with Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85u8dQnqJYgI"
   },
   "source": [
    "We need to transform test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ivFGZ5NHlkvg"
   },
   "outputs": [],
   "source": [
    "# Set nan to 0\n",
    "for value in ['nevents', 'ndays_act', 'nplay_video', 'nchapters']: # set nan to 0\n",
    "  edx_test_data[value] = edx_test_data[value].fillna(0)\n",
    "\n",
    "# Set nan to majority value\n",
    "for value2 in ['LoE_DI', 'gender']: \n",
    "  edx_test_data[value2] = edx_test_data[value2].fillna('o')\n",
    "\n",
    "average_yob = math.floor(edx_test_data['YoB'].mean()) * 1.0\n",
    "edx_test_data['YoB'] = edx_test_data['YoB'].fillna(average_yob)\n",
    "\n",
    "# Set nan to the mean value\n",
    "edx_test_data['grade'] = edx_test_data['grade'].str.strip()\n",
    "data = edx_test_data['grade'].tolist()\n",
    "sum = 0.0\n",
    "count = len(edx_test_data['grade'])\n",
    "minus = 0\n",
    "for value3 in data: \n",
    "  if isinstance(value3, str) and len(value3.strip()) != 0:\n",
    "    sum = sum + float(value3)\n",
    "  else: \n",
    "    minus = minus + 1\n",
    "average_grade = sum/(count - minus)\n",
    "edx_test_data['grade'] = pd.to_numeric(edx_test_data['grade'])\n",
    "edx_test_data['grade'] = edx_test_data['grade'].fillna(average_grade)\n",
    "\n",
    "edx_train_data['grade'] = edx_train_data['grade'].fillna(average_grade)\n",
    "\n",
    "# Create a new column of duration and set nan to the mean value\n",
    "sum = 0\n",
    "count = 0\n",
    "day_list = []\n",
    "for value4 in range(edx_test_data.shape[0]):\n",
    "  d1 = edx_test_data['start_time_DI'][value4]\n",
    "  d2 = edx_test_data['last_event_DI'][value4]\n",
    "  if isinstance(d2, str):\n",
    "    d1 = datetime.strptime(d1, '%m/%d/%y')\n",
    "    d2 = datetime.strptime(d2, '%m/%d/%y')\n",
    "    day = abs(d1 - d2).days\n",
    "    sum += day\n",
    "    count = count + 1\n",
    "    day_list.append(day)\n",
    "  else:\n",
    "    day_list.append(np.nan)\n",
    "average_duration = sum / count\n",
    "\n",
    "edx_test_data['duration'] = day_list\n",
    "edx_test_data['duration'] = edx_test_data['duration'].fillna(average_duration)\n",
    "edx_test_data = edx_test_data.drop(['start_time_DI', 'last_event_DI'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n-96ID1Om4eP"
   },
   "outputs": [],
   "source": [
    "user_id = edx_test_data['userid_DI']\n",
    "edx_test_data = pd.get_dummies(edx_test_data)\n",
    "edx_test_data = edx_test_data[new_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrGFylwpnvtT"
   },
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2aattBRKnohm",
    "outputId": "d67886b3-da38-4e0b-ab8c-36beb631017d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid_DI</th>\n",
       "      <th>certified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MHxPC130476531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MHxPC130559898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MHxPC130552712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MHxPC130394971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MHxPC130191077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userid_DI  certified\n",
       "0  MHxPC130476531          1\n",
       "1  MHxPC130559898          0\n",
       "2  MHxPC130552712          1\n",
       "3  MHxPC130394971          1\n",
       "4  MHxPC130191077          1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edx_test_data['certified'] = knn4_model.predict(edx_test_data)\n",
    "edx_test_data['userid_DI'] = user_id\n",
    "test_data = edx_test_data[['userid_DI', 'certified']]\n",
    "edx_test_data = edx_test_data[new_features]\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6SEqkirDSKxg"
   },
   "outputs": [],
   "source": [
    "test_data.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOPPhQMNQpdE"
   },
   "source": [
    "## Ethical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHdqkV2BmY6n"
   },
   "source": [
    "If the model is used to maximize profit, then it means this model aims to maximize 1. the money that can earn from each student and 2. the number of students. Then this model will not be fairpeople from high income backgrounds and people from backgrounds that value education / certificates very much will be over-represented. On the other hand, people from lower income backgrounds will be under-represented. In short, this program will correctly identify the student group that pays the most and has the most numbers, and then the rest will miss the opportunity. Accuracy will be high, but fairness will be low. Imagine people live in rich urban areas and people live in poor rural areaspeople in rich urban areas will get all the opportunities and people in poor rural areas will get little or none. This is not fair. \n",
    "\n",
    "Weve talked about tradeoff between accuracy and fairness in week 4, this is the same thing. Its just that in this case, higher accuracy means more profit and vice versa, so theres a tradeoff between profit and fairness. We should think of the opportunities the lower income groups could get before acting on this plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSE 416 HW6.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
